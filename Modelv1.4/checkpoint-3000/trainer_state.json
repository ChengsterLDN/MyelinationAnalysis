{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9761904761904763,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00992063492063492,
      "grad_norm": 1.3719464540481567,
      "learning_rate": 4.985119047619048e-05,
      "loss": 0.9941,
      "step": 10
    },
    {
      "epoch": 0.01984126984126984,
      "grad_norm": 1.2379059791564941,
      "learning_rate": 4.968584656084656e-05,
      "loss": 0.6145,
      "step": 20
    },
    {
      "epoch": 0.02976190476190476,
      "grad_norm": 1.8104429244995117,
      "learning_rate": 4.952050264550265e-05,
      "loss": 0.4881,
      "step": 30
    },
    {
      "epoch": 0.03968253968253968,
      "grad_norm": 1.202915072441101,
      "learning_rate": 4.9355158730158735e-05,
      "loss": 0.4633,
      "step": 40
    },
    {
      "epoch": 0.0496031746031746,
      "grad_norm": 3.3433964252471924,
      "learning_rate": 4.9189814814814815e-05,
      "loss": 0.3231,
      "step": 50
    },
    {
      "epoch": 0.05952380952380952,
      "grad_norm": 0.9038017392158508,
      "learning_rate": 4.90244708994709e-05,
      "loss": 0.3268,
      "step": 60
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 2.038198947906494,
      "learning_rate": 4.8859126984126984e-05,
      "loss": 0.2575,
      "step": 70
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 2.7609469890594482,
      "learning_rate": 4.869378306878307e-05,
      "loss": 0.2613,
      "step": 80
    },
    {
      "epoch": 0.08928571428571429,
      "grad_norm": 3.6198573112487793,
      "learning_rate": 4.852843915343915e-05,
      "loss": 0.3895,
      "step": 90
    },
    {
      "epoch": 0.0992063492063492,
      "grad_norm": 0.7911335229873657,
      "learning_rate": 4.836309523809524e-05,
      "loss": 0.2519,
      "step": 100
    },
    {
      "epoch": 0.10912698412698413,
      "grad_norm": 0.7651588916778564,
      "learning_rate": 4.819775132275133e-05,
      "loss": 0.2423,
      "step": 110
    },
    {
      "epoch": 0.11904761904761904,
      "grad_norm": 0.8101857900619507,
      "learning_rate": 4.803240740740741e-05,
      "loss": 0.3194,
      "step": 120
    },
    {
      "epoch": 0.12896825396825398,
      "grad_norm": 1.2528316974639893,
      "learning_rate": 4.7867063492063496e-05,
      "loss": 0.2383,
      "step": 130
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 0.9945611357688904,
      "learning_rate": 4.7701719576719577e-05,
      "loss": 0.3078,
      "step": 140
    },
    {
      "epoch": 0.1488095238095238,
      "grad_norm": 2.178577184677124,
      "learning_rate": 4.7536375661375664e-05,
      "loss": 0.2212,
      "step": 150
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 1.6458840370178223,
      "learning_rate": 4.7371031746031745e-05,
      "loss": 0.2925,
      "step": 160
    },
    {
      "epoch": 0.16865079365079366,
      "grad_norm": 0.43491241335868835,
      "learning_rate": 4.720568783068783e-05,
      "loss": 0.3086,
      "step": 170
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 4.1074323654174805,
      "learning_rate": 4.704034391534391e-05,
      "loss": 0.2965,
      "step": 180
    },
    {
      "epoch": 0.1884920634920635,
      "grad_norm": 2.8642311096191406,
      "learning_rate": 4.6875e-05,
      "loss": 0.2723,
      "step": 190
    },
    {
      "epoch": 0.1984126984126984,
      "grad_norm": 3.72428035736084,
      "learning_rate": 4.670965608465609e-05,
      "loss": 0.288,
      "step": 200
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.5513815879821777,
      "learning_rate": 4.654431216931217e-05,
      "loss": 0.1966,
      "step": 210
    },
    {
      "epoch": 0.21825396825396826,
      "grad_norm": 1.6815180778503418,
      "learning_rate": 4.637896825396826e-05,
      "loss": 0.2001,
      "step": 220
    },
    {
      "epoch": 0.22817460317460317,
      "grad_norm": 1.9883300065994263,
      "learning_rate": 4.621362433862434e-05,
      "loss": 0.3088,
      "step": 230
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 1.7422568798065186,
      "learning_rate": 4.6048280423280425e-05,
      "loss": 0.1754,
      "step": 240
    },
    {
      "epoch": 0.24801587301587302,
      "grad_norm": 5.885217189788818,
      "learning_rate": 4.5882936507936506e-05,
      "loss": 0.2632,
      "step": 250
    },
    {
      "epoch": 0.25793650793650796,
      "grad_norm": 3.6565613746643066,
      "learning_rate": 4.5717592592592594e-05,
      "loss": 0.2004,
      "step": 260
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 0.864947497844696,
      "learning_rate": 4.5552248677248675e-05,
      "loss": 0.1733,
      "step": 270
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.4264894723892212,
      "learning_rate": 4.538690476190476e-05,
      "loss": 0.2012,
      "step": 280
    },
    {
      "epoch": 0.2876984126984127,
      "grad_norm": 3.0318942070007324,
      "learning_rate": 4.522156084656085e-05,
      "loss": 0.2405,
      "step": 290
    },
    {
      "epoch": 0.2976190476190476,
      "grad_norm": 2.015702962875366,
      "learning_rate": 4.505621693121693e-05,
      "loss": 0.2608,
      "step": 300
    },
    {
      "epoch": 0.30753968253968256,
      "grad_norm": 2.5453720092773438,
      "learning_rate": 4.489087301587302e-05,
      "loss": 0.1919,
      "step": 310
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.011779546737671,
      "learning_rate": 4.47255291005291e-05,
      "loss": 0.158,
      "step": 320
    },
    {
      "epoch": 0.3273809523809524,
      "grad_norm": 3.1051385402679443,
      "learning_rate": 4.456018518518519e-05,
      "loss": 0.2216,
      "step": 330
    },
    {
      "epoch": 0.3373015873015873,
      "grad_norm": 1.918552041053772,
      "learning_rate": 4.439484126984127e-05,
      "loss": 0.1994,
      "step": 340
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 0.5273995995521545,
      "learning_rate": 4.4229497354497355e-05,
      "loss": 0.122,
      "step": 350
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 0.6884121894836426,
      "learning_rate": 4.406415343915344e-05,
      "loss": 0.184,
      "step": 360
    },
    {
      "epoch": 0.36706349206349204,
      "grad_norm": 2.3656935691833496,
      "learning_rate": 4.3898809523809523e-05,
      "loss": 0.2297,
      "step": 370
    },
    {
      "epoch": 0.376984126984127,
      "grad_norm": 2.9331512451171875,
      "learning_rate": 4.373346560846561e-05,
      "loss": 0.2267,
      "step": 380
    },
    {
      "epoch": 0.3869047619047619,
      "grad_norm": 1.8614566326141357,
      "learning_rate": 4.356812169312169e-05,
      "loss": 0.1711,
      "step": 390
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 1.0340700149536133,
      "learning_rate": 4.340277777777778e-05,
      "loss": 0.1456,
      "step": 400
    },
    {
      "epoch": 0.40674603174603174,
      "grad_norm": 0.6053463220596313,
      "learning_rate": 4.323743386243386e-05,
      "loss": 0.1799,
      "step": 410
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.6186718940734863,
      "learning_rate": 4.307208994708995e-05,
      "loss": 0.1781,
      "step": 420
    },
    {
      "epoch": 0.42658730158730157,
      "grad_norm": 2.79207706451416,
      "learning_rate": 4.290674603174603e-05,
      "loss": 0.2176,
      "step": 430
    },
    {
      "epoch": 0.4365079365079365,
      "grad_norm": 0.8014230728149414,
      "learning_rate": 4.2741402116402116e-05,
      "loss": 0.1968,
      "step": 440
    },
    {
      "epoch": 0.44642857142857145,
      "grad_norm": 2.2437567710876465,
      "learning_rate": 4.2576058201058204e-05,
      "loss": 0.2028,
      "step": 450
    },
    {
      "epoch": 0.45634920634920634,
      "grad_norm": 3.0505282878875732,
      "learning_rate": 4.2410714285714285e-05,
      "loss": 0.2338,
      "step": 460
    },
    {
      "epoch": 0.4662698412698413,
      "grad_norm": 1.1223489046096802,
      "learning_rate": 4.224537037037037e-05,
      "loss": 0.1366,
      "step": 470
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 4.450564861297607,
      "learning_rate": 4.208002645502645e-05,
      "loss": 0.1737,
      "step": 480
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 0.6995857954025269,
      "learning_rate": 4.191468253968254e-05,
      "loss": 0.1493,
      "step": 490
    },
    {
      "epoch": 0.49603174603174605,
      "grad_norm": 0.6147041320800781,
      "learning_rate": 4.174933862433862e-05,
      "loss": 0.1263,
      "step": 500
    },
    {
      "epoch": 0.5059523809523809,
      "grad_norm": 0.8544953465461731,
      "learning_rate": 4.158399470899471e-05,
      "loss": 0.1712,
      "step": 510
    },
    {
      "epoch": 0.5158730158730159,
      "grad_norm": 0.9247413873672485,
      "learning_rate": 4.14186507936508e-05,
      "loss": 0.144,
      "step": 520
    },
    {
      "epoch": 0.5257936507936508,
      "grad_norm": 2.4032070636749268,
      "learning_rate": 4.125330687830688e-05,
      "loss": 0.1912,
      "step": 530
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 2.98814058303833,
      "learning_rate": 4.1087962962962965e-05,
      "loss": 0.199,
      "step": 540
    },
    {
      "epoch": 0.5456349206349206,
      "grad_norm": 0.9423033595085144,
      "learning_rate": 4.0922619047619046e-05,
      "loss": 0.2026,
      "step": 550
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.5680019855499268,
      "learning_rate": 4.0757275132275133e-05,
      "loss": 0.1597,
      "step": 560
    },
    {
      "epoch": 0.5654761904761905,
      "grad_norm": 0.3485705852508545,
      "learning_rate": 4.0591931216931214e-05,
      "loss": 0.1156,
      "step": 570
    },
    {
      "epoch": 0.5753968253968254,
      "grad_norm": 2.15037202835083,
      "learning_rate": 4.04265873015873e-05,
      "loss": 0.2018,
      "step": 580
    },
    {
      "epoch": 0.5853174603174603,
      "grad_norm": 0.1341075897216797,
      "learning_rate": 4.026124338624338e-05,
      "loss": 0.0855,
      "step": 590
    },
    {
      "epoch": 0.5952380952380952,
      "grad_norm": 1.1490334272384644,
      "learning_rate": 4.009589947089947e-05,
      "loss": 0.2284,
      "step": 600
    },
    {
      "epoch": 0.6051587301587301,
      "grad_norm": 2.6367533206939697,
      "learning_rate": 3.993055555555556e-05,
      "loss": 0.1153,
      "step": 610
    },
    {
      "epoch": 0.6150793650793651,
      "grad_norm": 0.10449708253145218,
      "learning_rate": 3.976521164021164e-05,
      "loss": 0.1724,
      "step": 620
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.27132198214530945,
      "learning_rate": 3.9599867724867726e-05,
      "loss": 0.1256,
      "step": 630
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.30907630920410156,
      "learning_rate": 3.943452380952381e-05,
      "loss": 0.1851,
      "step": 640
    },
    {
      "epoch": 0.6448412698412699,
      "grad_norm": 4.131710052490234,
      "learning_rate": 3.9269179894179895e-05,
      "loss": 0.1456,
      "step": 650
    },
    {
      "epoch": 0.6547619047619048,
      "grad_norm": 2.613365650177002,
      "learning_rate": 3.9103835978835976e-05,
      "loss": 0.1707,
      "step": 660
    },
    {
      "epoch": 0.6646825396825397,
      "grad_norm": 2.1130526065826416,
      "learning_rate": 3.893849206349206e-05,
      "loss": 0.1795,
      "step": 670
    },
    {
      "epoch": 0.6746031746031746,
      "grad_norm": 1.5828524827957153,
      "learning_rate": 3.877314814814815e-05,
      "loss": 0.1297,
      "step": 680
    },
    {
      "epoch": 0.6845238095238095,
      "grad_norm": 3.466740369796753,
      "learning_rate": 3.860780423280423e-05,
      "loss": 0.1755,
      "step": 690
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 5.604875087738037,
      "learning_rate": 3.844246031746032e-05,
      "loss": 0.1935,
      "step": 700
    },
    {
      "epoch": 0.7043650793650794,
      "grad_norm": 2.5593626499176025,
      "learning_rate": 3.82771164021164e-05,
      "loss": 0.1192,
      "step": 710
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 3.2461647987365723,
      "learning_rate": 3.811177248677249e-05,
      "loss": 0.1875,
      "step": 720
    },
    {
      "epoch": 0.7242063492063492,
      "grad_norm": 0.4990682005882263,
      "learning_rate": 3.794642857142857e-05,
      "loss": 0.0866,
      "step": 730
    },
    {
      "epoch": 0.7341269841269841,
      "grad_norm": 0.07594171911478043,
      "learning_rate": 3.7781084656084656e-05,
      "loss": 0.0921,
      "step": 740
    },
    {
      "epoch": 0.7440476190476191,
      "grad_norm": 0.2641228139400482,
      "learning_rate": 3.7615740740740744e-05,
      "loss": 0.1771,
      "step": 750
    },
    {
      "epoch": 0.753968253968254,
      "grad_norm": 1.4916601181030273,
      "learning_rate": 3.7450396825396824e-05,
      "loss": 0.1221,
      "step": 760
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 0.13018600642681122,
      "learning_rate": 3.728505291005291e-05,
      "loss": 0.2582,
      "step": 770
    },
    {
      "epoch": 0.7738095238095238,
      "grad_norm": 3.3499596118927,
      "learning_rate": 3.711970899470899e-05,
      "loss": 0.1294,
      "step": 780
    },
    {
      "epoch": 0.7837301587301587,
      "grad_norm": 2.761120319366455,
      "learning_rate": 3.695436507936508e-05,
      "loss": 0.2102,
      "step": 790
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 5.302552223205566,
      "learning_rate": 3.678902116402117e-05,
      "loss": 0.1867,
      "step": 800
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 0.5618389844894409,
      "learning_rate": 3.662367724867725e-05,
      "loss": 0.1046,
      "step": 810
    },
    {
      "epoch": 0.8134920634920635,
      "grad_norm": 1.3859440088272095,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.1232,
      "step": 820
    },
    {
      "epoch": 0.8234126984126984,
      "grad_norm": 5.243439674377441,
      "learning_rate": 3.629298941798942e-05,
      "loss": 0.2123,
      "step": 830
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 3.28774094581604,
      "learning_rate": 3.6127645502645505e-05,
      "loss": 0.1462,
      "step": 840
    },
    {
      "epoch": 0.8432539682539683,
      "grad_norm": 0.245246022939682,
      "learning_rate": 3.5962301587301586e-05,
      "loss": 0.1259,
      "step": 850
    },
    {
      "epoch": 0.8531746031746031,
      "grad_norm": 2.248469591140747,
      "learning_rate": 3.579695767195767e-05,
      "loss": 0.134,
      "step": 860
    },
    {
      "epoch": 0.8630952380952381,
      "grad_norm": 0.23854048550128937,
      "learning_rate": 3.563161375661376e-05,
      "loss": 0.148,
      "step": 870
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 1.5220264196395874,
      "learning_rate": 3.546626984126984e-05,
      "loss": 0.108,
      "step": 880
    },
    {
      "epoch": 0.8829365079365079,
      "grad_norm": 0.084409698843956,
      "learning_rate": 3.530092592592593e-05,
      "loss": 0.1012,
      "step": 890
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 2.081437587738037,
      "learning_rate": 3.513558201058201e-05,
      "loss": 0.2269,
      "step": 900
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 3.7749485969543457,
      "learning_rate": 3.49702380952381e-05,
      "loss": 0.1469,
      "step": 910
    },
    {
      "epoch": 0.9126984126984127,
      "grad_norm": 2.906954765319824,
      "learning_rate": 3.4804894179894185e-05,
      "loss": 0.2006,
      "step": 920
    },
    {
      "epoch": 0.9226190476190477,
      "grad_norm": 1.0022488832473755,
      "learning_rate": 3.4639550264550266e-05,
      "loss": 0.1111,
      "step": 930
    },
    {
      "epoch": 0.9325396825396826,
      "grad_norm": 2.0710771083831787,
      "learning_rate": 3.4474206349206354e-05,
      "loss": 0.1074,
      "step": 940
    },
    {
      "epoch": 0.9424603174603174,
      "grad_norm": 0.38613054156303406,
      "learning_rate": 3.4308862433862434e-05,
      "loss": 0.1262,
      "step": 950
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.36195993423461914,
      "learning_rate": 3.414351851851852e-05,
      "loss": 0.1678,
      "step": 960
    },
    {
      "epoch": 0.9623015873015873,
      "grad_norm": 0.8114310503005981,
      "learning_rate": 3.397817460317461e-05,
      "loss": 0.109,
      "step": 970
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 3.552114963531494,
      "learning_rate": 3.381283068783069e-05,
      "loss": 0.1674,
      "step": 980
    },
    {
      "epoch": 0.9821428571428571,
      "grad_norm": 0.8657990097999573,
      "learning_rate": 3.364748677248678e-05,
      "loss": 0.1636,
      "step": 990
    },
    {
      "epoch": 0.9920634920634921,
      "grad_norm": 1.0813343524932861,
      "learning_rate": 3.348214285714286e-05,
      "loss": 0.1162,
      "step": 1000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.935,
      "eval_loss": 0.18352161347866058,
      "eval_runtime": 337.7978,
      "eval_samples_per_second": 10.657,
      "eval_steps_per_second": 1.332,
      "step": 1008
    },
    {
      "epoch": 1.001984126984127,
      "grad_norm": 2.6641106605529785,
      "learning_rate": 3.3316798941798946e-05,
      "loss": 0.111,
      "step": 1010
    },
    {
      "epoch": 1.0119047619047619,
      "grad_norm": 0.05984215438365936,
      "learning_rate": 3.315145502645503e-05,
      "loss": 0.0551,
      "step": 1020
    },
    {
      "epoch": 1.0218253968253967,
      "grad_norm": 2.750338554382324,
      "learning_rate": 3.2986111111111115e-05,
      "loss": 0.0872,
      "step": 1030
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 3.121300458908081,
      "learning_rate": 3.28207671957672e-05,
      "loss": 0.0855,
      "step": 1040
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.5413240194320679,
      "learning_rate": 3.265542328042328e-05,
      "loss": 0.111,
      "step": 1050
    },
    {
      "epoch": 1.0515873015873016,
      "grad_norm": 2.6723039150238037,
      "learning_rate": 3.249007936507937e-05,
      "loss": 0.0841,
      "step": 1060
    },
    {
      "epoch": 1.0615079365079365,
      "grad_norm": 0.6056667566299438,
      "learning_rate": 3.232473544973545e-05,
      "loss": 0.0595,
      "step": 1070
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 2.3322958946228027,
      "learning_rate": 3.215939153439154e-05,
      "loss": 0.1052,
      "step": 1080
    },
    {
      "epoch": 1.0813492063492063,
      "grad_norm": 0.3279564380645752,
      "learning_rate": 3.199404761904762e-05,
      "loss": 0.0707,
      "step": 1090
    },
    {
      "epoch": 1.0912698412698412,
      "grad_norm": 0.15563958883285522,
      "learning_rate": 3.182870370370371e-05,
      "loss": 0.06,
      "step": 1100
    },
    {
      "epoch": 1.1011904761904763,
      "grad_norm": 0.06619957834482193,
      "learning_rate": 3.1663359788359795e-05,
      "loss": 0.0509,
      "step": 1110
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.725649118423462,
      "learning_rate": 3.1498015873015876e-05,
      "loss": 0.0648,
      "step": 1120
    },
    {
      "epoch": 1.121031746031746,
      "grad_norm": 0.0885264128446579,
      "learning_rate": 3.1332671957671964e-05,
      "loss": 0.0988,
      "step": 1130
    },
    {
      "epoch": 1.130952380952381,
      "grad_norm": 0.09525419026613235,
      "learning_rate": 3.1167328042328044e-05,
      "loss": 0.061,
      "step": 1140
    },
    {
      "epoch": 1.1408730158730158,
      "grad_norm": 0.20316679775714874,
      "learning_rate": 3.100198412698413e-05,
      "loss": 0.0406,
      "step": 1150
    },
    {
      "epoch": 1.1507936507936507,
      "grad_norm": 0.5155660510063171,
      "learning_rate": 3.083664021164021e-05,
      "loss": 0.0883,
      "step": 1160
    },
    {
      "epoch": 1.1607142857142858,
      "grad_norm": 0.11247237026691437,
      "learning_rate": 3.06712962962963e-05,
      "loss": 0.0492,
      "step": 1170
    },
    {
      "epoch": 1.1706349206349207,
      "grad_norm": 1.7112199068069458,
      "learning_rate": 3.0505952380952385e-05,
      "loss": 0.0825,
      "step": 1180
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 0.05518156662583351,
      "learning_rate": 3.0340608465608465e-05,
      "loss": 0.0756,
      "step": 1190
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.045167069882154465,
      "learning_rate": 3.0175264550264553e-05,
      "loss": 0.0649,
      "step": 1200
    },
    {
      "epoch": 1.2003968253968254,
      "grad_norm": 6.698178291320801,
      "learning_rate": 3.0009920634920634e-05,
      "loss": 0.0996,
      "step": 1210
    },
    {
      "epoch": 1.2103174603174602,
      "grad_norm": 0.3445168733596802,
      "learning_rate": 2.984457671957672e-05,
      "loss": 0.0204,
      "step": 1220
    },
    {
      "epoch": 1.2202380952380953,
      "grad_norm": 4.044685363769531,
      "learning_rate": 2.9679232804232802e-05,
      "loss": 0.1113,
      "step": 1230
    },
    {
      "epoch": 1.2301587301587302,
      "grad_norm": 0.037860043346881866,
      "learning_rate": 2.951388888888889e-05,
      "loss": 0.0677,
      "step": 1240
    },
    {
      "epoch": 1.2400793650793651,
      "grad_norm": 9.59068489074707,
      "learning_rate": 2.9348544973544974e-05,
      "loss": 0.1517,
      "step": 1250
    },
    {
      "epoch": 1.25,
      "grad_norm": 5.525794506072998,
      "learning_rate": 2.9183201058201058e-05,
      "loss": 0.0847,
      "step": 1260
    },
    {
      "epoch": 1.2599206349206349,
      "grad_norm": 2.886760711669922,
      "learning_rate": 2.9017857142857146e-05,
      "loss": 0.077,
      "step": 1270
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.06335362792015076,
      "learning_rate": 2.8852513227513227e-05,
      "loss": 0.0531,
      "step": 1280
    },
    {
      "epoch": 1.2797619047619047,
      "grad_norm": 0.15907207131385803,
      "learning_rate": 2.8687169312169314e-05,
      "loss": 0.0919,
      "step": 1290
    },
    {
      "epoch": 1.2896825396825398,
      "grad_norm": 0.6899881362915039,
      "learning_rate": 2.8521825396825395e-05,
      "loss": 0.027,
      "step": 1300
    },
    {
      "epoch": 1.2996031746031746,
      "grad_norm": 1.6225579977035522,
      "learning_rate": 2.8356481481481483e-05,
      "loss": 0.0906,
      "step": 1310
    },
    {
      "epoch": 1.3095238095238095,
      "grad_norm": 0.10980452597141266,
      "learning_rate": 2.8191137566137567e-05,
      "loss": 0.0366,
      "step": 1320
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 0.08815739303827286,
      "learning_rate": 2.802579365079365e-05,
      "loss": 0.0439,
      "step": 1330
    },
    {
      "epoch": 1.3293650793650793,
      "grad_norm": 0.046432241797447205,
      "learning_rate": 2.7860449735449735e-05,
      "loss": 0.0663,
      "step": 1340
    },
    {
      "epoch": 1.3392857142857144,
      "grad_norm": 5.183873176574707,
      "learning_rate": 2.769510582010582e-05,
      "loss": 0.1175,
      "step": 1350
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 1.345420241355896,
      "learning_rate": 2.7529761904761907e-05,
      "loss": 0.0613,
      "step": 1360
    },
    {
      "epoch": 1.3591269841269842,
      "grad_norm": 0.09100791066884995,
      "learning_rate": 2.736441798941799e-05,
      "loss": 0.0532,
      "step": 1370
    },
    {
      "epoch": 1.369047619047619,
      "grad_norm": 0.0635804533958435,
      "learning_rate": 2.7199074074074076e-05,
      "loss": 0.0584,
      "step": 1380
    },
    {
      "epoch": 1.378968253968254,
      "grad_norm": 0.9684438705444336,
      "learning_rate": 2.703373015873016e-05,
      "loss": 0.1432,
      "step": 1390
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.5663745999336243,
      "learning_rate": 2.6868386243386244e-05,
      "loss": 0.0679,
      "step": 1400
    },
    {
      "epoch": 1.3988095238095237,
      "grad_norm": 0.3949184715747833,
      "learning_rate": 2.6703042328042328e-05,
      "loss": 0.0278,
      "step": 1410
    },
    {
      "epoch": 1.4087301587301586,
      "grad_norm": 0.20060431957244873,
      "learning_rate": 2.6537698412698416e-05,
      "loss": 0.0327,
      "step": 1420
    },
    {
      "epoch": 1.4186507936507937,
      "grad_norm": 3.2071316242218018,
      "learning_rate": 2.63723544973545e-05,
      "loss": 0.0449,
      "step": 1430
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 6.68994140625,
      "learning_rate": 2.6207010582010584e-05,
      "loss": 0.1049,
      "step": 1440
    },
    {
      "epoch": 1.4384920634920635,
      "grad_norm": 0.0878763273358345,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.0793,
      "step": 1450
    },
    {
      "epoch": 1.4484126984126984,
      "grad_norm": 3.722641706466675,
      "learning_rate": 2.5876322751322753e-05,
      "loss": 0.1205,
      "step": 1460
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.2635596990585327,
      "learning_rate": 2.5710978835978837e-05,
      "loss": 0.0651,
      "step": 1470
    },
    {
      "epoch": 1.4682539682539684,
      "grad_norm": 0.04513042792677879,
      "learning_rate": 2.554563492063492e-05,
      "loss": 0.0172,
      "step": 1480
    },
    {
      "epoch": 1.4781746031746033,
      "grad_norm": 0.21350228786468506,
      "learning_rate": 2.538029100529101e-05,
      "loss": 0.0871,
      "step": 1490
    },
    {
      "epoch": 1.4880952380952381,
      "grad_norm": 6.689230918884277,
      "learning_rate": 2.521494708994709e-05,
      "loss": 0.09,
      "step": 1500
    },
    {
      "epoch": 1.498015873015873,
      "grad_norm": 0.30477917194366455,
      "learning_rate": 2.5049603174603177e-05,
      "loss": 0.0713,
      "step": 1510
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 4.985591411590576,
      "learning_rate": 2.488425925925926e-05,
      "loss": 0.049,
      "step": 1520
    },
    {
      "epoch": 1.5178571428571428,
      "grad_norm": 0.040600262582302094,
      "learning_rate": 2.4718915343915345e-05,
      "loss": 0.0571,
      "step": 1530
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.07370278239250183,
      "learning_rate": 2.455357142857143e-05,
      "loss": 0.0405,
      "step": 1540
    },
    {
      "epoch": 1.5376984126984126,
      "grad_norm": 0.6535500884056091,
      "learning_rate": 2.4388227513227517e-05,
      "loss": 0.0415,
      "step": 1550
    },
    {
      "epoch": 1.5476190476190477,
      "grad_norm": 0.2709926664829254,
      "learning_rate": 2.42228835978836e-05,
      "loss": 0.0552,
      "step": 1560
    },
    {
      "epoch": 1.5575396825396826,
      "grad_norm": 0.040491074323654175,
      "learning_rate": 2.4057539682539686e-05,
      "loss": 0.0315,
      "step": 1570
    },
    {
      "epoch": 1.5674603174603174,
      "grad_norm": 0.05723107233643532,
      "learning_rate": 2.389219576719577e-05,
      "loss": 0.0221,
      "step": 1580
    },
    {
      "epoch": 1.5773809523809523,
      "grad_norm": 0.035011980682611465,
      "learning_rate": 2.3726851851851854e-05,
      "loss": 0.0676,
      "step": 1590
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.052399445325136185,
      "learning_rate": 2.3561507936507938e-05,
      "loss": 0.0536,
      "step": 1600
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 8.506649017333984,
      "learning_rate": 2.3396164021164022e-05,
      "loss": 0.0609,
      "step": 1610
    },
    {
      "epoch": 1.6071428571428572,
      "grad_norm": 0.6980825662612915,
      "learning_rate": 2.3230820105820107e-05,
      "loss": 0.0276,
      "step": 1620
    },
    {
      "epoch": 1.617063492063492,
      "grad_norm": 8.876601219177246,
      "learning_rate": 2.3065476190476194e-05,
      "loss": 0.0854,
      "step": 1630
    },
    {
      "epoch": 1.626984126984127,
      "grad_norm": 0.03290814906358719,
      "learning_rate": 2.290013227513228e-05,
      "loss": 0.0285,
      "step": 1640
    },
    {
      "epoch": 1.6369047619047619,
      "grad_norm": 0.12953603267669678,
      "learning_rate": 2.2734788359788363e-05,
      "loss": 0.0451,
      "step": 1650
    },
    {
      "epoch": 1.6468253968253967,
      "grad_norm": 0.027155635878443718,
      "learning_rate": 2.2569444444444447e-05,
      "loss": 0.045,
      "step": 1660
    },
    {
      "epoch": 1.6567460317460316,
      "grad_norm": 1.191678524017334,
      "learning_rate": 2.240410052910053e-05,
      "loss": 0.0976,
      "step": 1670
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.0744284987449646,
      "learning_rate": 2.2238756613756615e-05,
      "loss": 0.0149,
      "step": 1680
    },
    {
      "epoch": 1.6765873015873016,
      "grad_norm": 0.02525665983557701,
      "learning_rate": 2.20734126984127e-05,
      "loss": 0.0079,
      "step": 1690
    },
    {
      "epoch": 1.6865079365079365,
      "grad_norm": 0.28873544931411743,
      "learning_rate": 2.1908068783068784e-05,
      "loss": 0.0463,
      "step": 1700
    },
    {
      "epoch": 1.6964285714285714,
      "grad_norm": 1.3526880741119385,
      "learning_rate": 2.1742724867724868e-05,
      "loss": 0.0224,
      "step": 1710
    },
    {
      "epoch": 1.7063492063492065,
      "grad_norm": 0.08722732961177826,
      "learning_rate": 2.1577380952380955e-05,
      "loss": 0.0476,
      "step": 1720
    },
    {
      "epoch": 1.7162698412698414,
      "grad_norm": 0.026476038619875908,
      "learning_rate": 2.141203703703704e-05,
      "loss": 0.024,
      "step": 1730
    },
    {
      "epoch": 1.7261904761904763,
      "grad_norm": 8.292471885681152,
      "learning_rate": 2.1246693121693124e-05,
      "loss": 0.0365,
      "step": 1740
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 0.03765282779932022,
      "learning_rate": 2.1081349206349208e-05,
      "loss": 0.0428,
      "step": 1750
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.06874677538871765,
      "learning_rate": 2.0916005291005292e-05,
      "loss": 0.0139,
      "step": 1760
    },
    {
      "epoch": 1.755952380952381,
      "grad_norm": 0.05351311340928078,
      "learning_rate": 2.0750661375661376e-05,
      "loss": 0.0302,
      "step": 1770
    },
    {
      "epoch": 1.7658730158730158,
      "grad_norm": 0.05602902173995972,
      "learning_rate": 2.058531746031746e-05,
      "loss": 0.0141,
      "step": 1780
    },
    {
      "epoch": 1.7757936507936507,
      "grad_norm": 0.025874976068735123,
      "learning_rate": 2.0419973544973545e-05,
      "loss": 0.0325,
      "step": 1790
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 0.4444032609462738,
      "learning_rate": 2.0254629629629632e-05,
      "loss": 0.0212,
      "step": 1800
    },
    {
      "epoch": 1.7956349206349205,
      "grad_norm": 0.04365108162164688,
      "learning_rate": 2.0089285714285717e-05,
      "loss": 0.021,
      "step": 1810
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.02829129993915558,
      "learning_rate": 1.99239417989418e-05,
      "loss": 0.0651,
      "step": 1820
    },
    {
      "epoch": 1.8154761904761905,
      "grad_norm": 0.8138101100921631,
      "learning_rate": 1.9758597883597885e-05,
      "loss": 0.0197,
      "step": 1830
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 0.2588701546192169,
      "learning_rate": 1.959325396825397e-05,
      "loss": 0.0094,
      "step": 1840
    },
    {
      "epoch": 1.8353174603174605,
      "grad_norm": 2.119145631790161,
      "learning_rate": 1.9427910052910053e-05,
      "loss": 0.0247,
      "step": 1850
    },
    {
      "epoch": 1.8452380952380953,
      "grad_norm": 0.04845333844423294,
      "learning_rate": 1.9262566137566138e-05,
      "loss": 0.0384,
      "step": 1860
    },
    {
      "epoch": 1.8551587301587302,
      "grad_norm": 1.2099987268447876,
      "learning_rate": 1.9097222222222222e-05,
      "loss": 0.0186,
      "step": 1870
    },
    {
      "epoch": 1.8650793650793651,
      "grad_norm": 7.193816661834717,
      "learning_rate": 1.893187830687831e-05,
      "loss": 0.0791,
      "step": 1880
    },
    {
      "epoch": 1.875,
      "grad_norm": 3.8149399757385254,
      "learning_rate": 1.8766534391534394e-05,
      "loss": 0.0111,
      "step": 1890
    },
    {
      "epoch": 1.8849206349206349,
      "grad_norm": 0.027651647105813026,
      "learning_rate": 1.8601190476190478e-05,
      "loss": 0.0124,
      "step": 1900
    },
    {
      "epoch": 1.8948412698412698,
      "grad_norm": 0.03417171910405159,
      "learning_rate": 1.8435846560846562e-05,
      "loss": 0.0178,
      "step": 1910
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 2.5916733741760254,
      "learning_rate": 1.8270502645502646e-05,
      "loss": 0.0683,
      "step": 1920
    },
    {
      "epoch": 1.9146825396825395,
      "grad_norm": 0.05737987905740738,
      "learning_rate": 1.810515873015873e-05,
      "loss": 0.0779,
      "step": 1930
    },
    {
      "epoch": 1.9246031746031746,
      "grad_norm": 2.1861586570739746,
      "learning_rate": 1.7939814814814815e-05,
      "loss": 0.0398,
      "step": 1940
    },
    {
      "epoch": 1.9345238095238095,
      "grad_norm": 0.02303178608417511,
      "learning_rate": 1.77744708994709e-05,
      "loss": 0.0116,
      "step": 1950
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.03566618636250496,
      "learning_rate": 1.7609126984126986e-05,
      "loss": 0.0107,
      "step": 1960
    },
    {
      "epoch": 1.9543650793650795,
      "grad_norm": 0.254242867231369,
      "learning_rate": 1.744378306878307e-05,
      "loss": 0.0197,
      "step": 1970
    },
    {
      "epoch": 1.9642857142857144,
      "grad_norm": 0.028539173305034637,
      "learning_rate": 1.7278439153439155e-05,
      "loss": 0.0251,
      "step": 1980
    },
    {
      "epoch": 1.9742063492063493,
      "grad_norm": 0.028155066072940826,
      "learning_rate": 1.711309523809524e-05,
      "loss": 0.0188,
      "step": 1990
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 0.16760952770709991,
      "learning_rate": 1.6947751322751323e-05,
      "loss": 0.0103,
      "step": 2000
    },
    {
      "epoch": 1.994047619047619,
      "grad_norm": 1.2936421632766724,
      "learning_rate": 1.6782407407407408e-05,
      "loss": 0.0212,
      "step": 2010
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9363888888888889,
      "eval_loss": 0.25687429308891296,
      "eval_runtime": 320.9864,
      "eval_samples_per_second": 11.215,
      "eval_steps_per_second": 1.402,
      "step": 2016
    },
    {
      "epoch": 2.003968253968254,
      "grad_norm": 0.027065297588706017,
      "learning_rate": 1.6617063492063492e-05,
      "loss": 0.0325,
      "step": 2020
    },
    {
      "epoch": 2.013888888888889,
      "grad_norm": 0.11986981332302094,
      "learning_rate": 1.6451719576719576e-05,
      "loss": 0.0104,
      "step": 2030
    },
    {
      "epoch": 2.0238095238095237,
      "grad_norm": 3.2220308780670166,
      "learning_rate": 1.6286375661375664e-05,
      "loss": 0.0082,
      "step": 2040
    },
    {
      "epoch": 2.0337301587301586,
      "grad_norm": 0.046483542770147324,
      "learning_rate": 1.6121031746031748e-05,
      "loss": 0.0076,
      "step": 2050
    },
    {
      "epoch": 2.0436507936507935,
      "grad_norm": 0.020458657294511795,
      "learning_rate": 1.5955687830687832e-05,
      "loss": 0.0229,
      "step": 2060
    },
    {
      "epoch": 2.0535714285714284,
      "grad_norm": 0.0244448222219944,
      "learning_rate": 1.5790343915343916e-05,
      "loss": 0.0059,
      "step": 2070
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.03724638372659683,
      "learning_rate": 1.5625e-05,
      "loss": 0.0136,
      "step": 2080
    },
    {
      "epoch": 2.0734126984126986,
      "grad_norm": 0.1046915352344513,
      "learning_rate": 1.5459656084656085e-05,
      "loss": 0.0063,
      "step": 2090
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.035344284027814865,
      "learning_rate": 1.529431216931217e-05,
      "loss": 0.0068,
      "step": 2100
    },
    {
      "epoch": 2.0932539682539684,
      "grad_norm": 0.23090499639511108,
      "learning_rate": 1.5128968253968253e-05,
      "loss": 0.0073,
      "step": 2110
    },
    {
      "epoch": 2.1031746031746033,
      "grad_norm": 0.034937694668769836,
      "learning_rate": 1.4963624338624339e-05,
      "loss": 0.0055,
      "step": 2120
    },
    {
      "epoch": 2.113095238095238,
      "grad_norm": 0.029459459707140923,
      "learning_rate": 1.4798280423280425e-05,
      "loss": 0.0063,
      "step": 2130
    },
    {
      "epoch": 2.123015873015873,
      "grad_norm": 0.049020349979400635,
      "learning_rate": 1.4632936507936509e-05,
      "loss": 0.0067,
      "step": 2140
    },
    {
      "epoch": 2.132936507936508,
      "grad_norm": 0.11367101222276688,
      "learning_rate": 1.4467592592592593e-05,
      "loss": 0.0109,
      "step": 2150
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.03602764755487442,
      "learning_rate": 1.4302248677248677e-05,
      "loss": 0.0062,
      "step": 2160
    },
    {
      "epoch": 2.1527777777777777,
      "grad_norm": 0.020394790917634964,
      "learning_rate": 1.4136904761904762e-05,
      "loss": 0.0117,
      "step": 2170
    },
    {
      "epoch": 2.1626984126984126,
      "grad_norm": 0.13121619820594788,
      "learning_rate": 1.3971560846560847e-05,
      "loss": 0.0052,
      "step": 2180
    },
    {
      "epoch": 2.1726190476190474,
      "grad_norm": 0.07872474938631058,
      "learning_rate": 1.3806216931216932e-05,
      "loss": 0.0053,
      "step": 2190
    },
    {
      "epoch": 2.1825396825396823,
      "grad_norm": 0.03367788344621658,
      "learning_rate": 1.3640873015873016e-05,
      "loss": 0.0057,
      "step": 2200
    },
    {
      "epoch": 2.1924603174603177,
      "grad_norm": 0.04177607223391533,
      "learning_rate": 1.3475529100529102e-05,
      "loss": 0.0081,
      "step": 2210
    },
    {
      "epoch": 2.2023809523809526,
      "grad_norm": 0.02390957623720169,
      "learning_rate": 1.3310185185185186e-05,
      "loss": 0.0063,
      "step": 2220
    },
    {
      "epoch": 2.2123015873015874,
      "grad_norm": 0.04043128713965416,
      "learning_rate": 1.314484126984127e-05,
      "loss": 0.0055,
      "step": 2230
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.025672554969787598,
      "learning_rate": 1.2979497354497356e-05,
      "loss": 0.0051,
      "step": 2240
    },
    {
      "epoch": 2.232142857142857,
      "grad_norm": 0.018538406118750572,
      "learning_rate": 1.281415343915344e-05,
      "loss": 0.0044,
      "step": 2250
    },
    {
      "epoch": 2.242063492063492,
      "grad_norm": 0.020378081128001213,
      "learning_rate": 1.2648809523809524e-05,
      "loss": 0.0114,
      "step": 2260
    },
    {
      "epoch": 2.251984126984127,
      "grad_norm": 0.029930563643574715,
      "learning_rate": 1.248346560846561e-05,
      "loss": 0.0121,
      "step": 2270
    },
    {
      "epoch": 2.261904761904762,
      "grad_norm": 0.15055705606937408,
      "learning_rate": 1.2318121693121695e-05,
      "loss": 0.0069,
      "step": 2280
    },
    {
      "epoch": 2.2718253968253967,
      "grad_norm": 0.06252049654722214,
      "learning_rate": 1.2152777777777779e-05,
      "loss": 0.0213,
      "step": 2290
    },
    {
      "epoch": 2.2817460317460316,
      "grad_norm": 0.023812150582671165,
      "learning_rate": 1.1987433862433863e-05,
      "loss": 0.0049,
      "step": 2300
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.11013548821210861,
      "learning_rate": 1.1822089947089949e-05,
      "loss": 0.0102,
      "step": 2310
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.04310449957847595,
      "learning_rate": 1.1656746031746033e-05,
      "loss": 0.0053,
      "step": 2320
    },
    {
      "epoch": 2.3115079365079367,
      "grad_norm": 0.05318806692957878,
      "learning_rate": 1.1491402116402117e-05,
      "loss": 0.0049,
      "step": 2330
    },
    {
      "epoch": 2.3214285714285716,
      "grad_norm": 0.04676302894949913,
      "learning_rate": 1.1326058201058202e-05,
      "loss": 0.0052,
      "step": 2340
    },
    {
      "epoch": 2.3313492063492065,
      "grad_norm": 0.029621673747897148,
      "learning_rate": 1.1160714285714287e-05,
      "loss": 0.0111,
      "step": 2350
    },
    {
      "epoch": 2.3412698412698414,
      "grad_norm": 0.017752699553966522,
      "learning_rate": 1.0995370370370372e-05,
      "loss": 0.005,
      "step": 2360
    },
    {
      "epoch": 2.3511904761904763,
      "grad_norm": 0.041374243795871735,
      "learning_rate": 1.0830026455026456e-05,
      "loss": 0.0062,
      "step": 2370
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 0.025905536487698555,
      "learning_rate": 1.066468253968254e-05,
      "loss": 0.0123,
      "step": 2380
    },
    {
      "epoch": 2.371031746031746,
      "grad_norm": 0.016977522522211075,
      "learning_rate": 1.0499338624338626e-05,
      "loss": 0.0054,
      "step": 2390
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.020313536748290062,
      "learning_rate": 1.033399470899471e-05,
      "loss": 0.0051,
      "step": 2400
    },
    {
      "epoch": 2.390873015873016,
      "grad_norm": 0.9128814339637756,
      "learning_rate": 1.0168650793650794e-05,
      "loss": 0.0057,
      "step": 2410
    },
    {
      "epoch": 2.4007936507936507,
      "grad_norm": 0.017117438837885857,
      "learning_rate": 1.0003306878306879e-05,
      "loss": 0.0258,
      "step": 2420
    },
    {
      "epoch": 2.4107142857142856,
      "grad_norm": 0.024592017754912376,
      "learning_rate": 9.837962962962964e-06,
      "loss": 0.0048,
      "step": 2430
    },
    {
      "epoch": 2.4206349206349205,
      "grad_norm": 0.023488927632570267,
      "learning_rate": 9.672619047619049e-06,
      "loss": 0.0054,
      "step": 2440
    },
    {
      "epoch": 2.4305555555555554,
      "grad_norm": 0.01570260338485241,
      "learning_rate": 9.507275132275133e-06,
      "loss": 0.0359,
      "step": 2450
    },
    {
      "epoch": 2.4404761904761907,
      "grad_norm": 0.01954435184597969,
      "learning_rate": 9.341931216931217e-06,
      "loss": 0.0042,
      "step": 2460
    },
    {
      "epoch": 2.4503968253968256,
      "grad_norm": 0.02129535563290119,
      "learning_rate": 9.176587301587301e-06,
      "loss": 0.0041,
      "step": 2470
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 0.023499440401792526,
      "learning_rate": 9.011243386243387e-06,
      "loss": 0.0045,
      "step": 2480
    },
    {
      "epoch": 2.4702380952380953,
      "grad_norm": 0.023410771042108536,
      "learning_rate": 8.845899470899471e-06,
      "loss": 0.0052,
      "step": 2490
    },
    {
      "epoch": 2.4801587301587302,
      "grad_norm": 0.019125329330563545,
      "learning_rate": 8.680555555555556e-06,
      "loss": 0.0041,
      "step": 2500
    },
    {
      "epoch": 2.490079365079365,
      "grad_norm": 0.018384909257292747,
      "learning_rate": 8.51521164021164e-06,
      "loss": 0.0047,
      "step": 2510
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.01634078659117222,
      "learning_rate": 8.349867724867726e-06,
      "loss": 0.0048,
      "step": 2520
    },
    {
      "epoch": 2.509920634920635,
      "grad_norm": 0.015923500061035156,
      "learning_rate": 8.18452380952381e-06,
      "loss": 0.0045,
      "step": 2530
    },
    {
      "epoch": 2.5198412698412698,
      "grad_norm": 0.024090027436614037,
      "learning_rate": 8.019179894179894e-06,
      "loss": 0.0045,
      "step": 2540
    },
    {
      "epoch": 2.5297619047619047,
      "grad_norm": 0.019038334488868713,
      "learning_rate": 7.853835978835978e-06,
      "loss": 0.005,
      "step": 2550
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.024733146652579308,
      "learning_rate": 7.688492063492064e-06,
      "loss": 0.0043,
      "step": 2560
    },
    {
      "epoch": 2.549603174603175,
      "grad_norm": 0.022513052448630333,
      "learning_rate": 7.523148148148148e-06,
      "loss": 0.005,
      "step": 2570
    },
    {
      "epoch": 2.5595238095238093,
      "grad_norm": 0.018783031031489372,
      "learning_rate": 7.3578042328042326e-06,
      "loss": 0.0255,
      "step": 2580
    },
    {
      "epoch": 2.5694444444444446,
      "grad_norm": 0.044333308935165405,
      "learning_rate": 7.192460317460318e-06,
      "loss": 0.0117,
      "step": 2590
    },
    {
      "epoch": 2.5793650793650795,
      "grad_norm": 0.014781572856009007,
      "learning_rate": 7.027116402116403e-06,
      "loss": 0.0203,
      "step": 2600
    },
    {
      "epoch": 2.5892857142857144,
      "grad_norm": 0.019703464582562447,
      "learning_rate": 6.861772486772487e-06,
      "loss": 0.0044,
      "step": 2610
    },
    {
      "epoch": 2.5992063492063493,
      "grad_norm": 0.022980166599154472,
      "learning_rate": 6.696428571428572e-06,
      "loss": 0.0038,
      "step": 2620
    },
    {
      "epoch": 2.609126984126984,
      "grad_norm": 0.06192623823881149,
      "learning_rate": 6.531084656084656e-06,
      "loss": 0.0042,
      "step": 2630
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.026080740615725517,
      "learning_rate": 6.365740740740741e-06,
      "loss": 0.0044,
      "step": 2640
    },
    {
      "epoch": 2.628968253968254,
      "grad_norm": 0.019699271768331528,
      "learning_rate": 6.200396825396826e-06,
      "loss": 0.0039,
      "step": 2650
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 0.021419430151581764,
      "learning_rate": 6.0350529100529104e-06,
      "loss": 0.0045,
      "step": 2660
    },
    {
      "epoch": 2.6488095238095237,
      "grad_norm": 0.017605692148208618,
      "learning_rate": 5.8697089947089955e-06,
      "loss": 0.0042,
      "step": 2670
    },
    {
      "epoch": 2.6587301587301586,
      "grad_norm": 0.014534536749124527,
      "learning_rate": 5.70436507936508e-06,
      "loss": 0.0041,
      "step": 2680
    },
    {
      "epoch": 2.6686507936507935,
      "grad_norm": 0.019880540668964386,
      "learning_rate": 5.539021164021165e-06,
      "loss": 0.0041,
      "step": 2690
    },
    {
      "epoch": 2.678571428571429,
      "grad_norm": 0.01586334779858589,
      "learning_rate": 5.373677248677249e-06,
      "loss": 0.0036,
      "step": 2700
    },
    {
      "epoch": 2.6884920634920633,
      "grad_norm": 0.03138232231140137,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.0047,
      "step": 2710
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 0.01772100292146206,
      "learning_rate": 5.042989417989418e-06,
      "loss": 0.0199,
      "step": 2720
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 0.015092807821929455,
      "learning_rate": 4.877645502645503e-06,
      "loss": 0.0038,
      "step": 2730
    },
    {
      "epoch": 2.7182539682539684,
      "grad_norm": 0.014362339861690998,
      "learning_rate": 4.7123015873015875e-06,
      "loss": 0.0043,
      "step": 2740
    },
    {
      "epoch": 2.7281746031746033,
      "grad_norm": 0.35115674138069153,
      "learning_rate": 4.5469576719576725e-06,
      "loss": 0.0054,
      "step": 2750
    },
    {
      "epoch": 2.738095238095238,
      "grad_norm": 0.020482445135712624,
      "learning_rate": 4.381613756613757e-06,
      "loss": 0.0048,
      "step": 2760
    },
    {
      "epoch": 2.748015873015873,
      "grad_norm": 0.02142156846821308,
      "learning_rate": 4.216269841269842e-06,
      "loss": 0.004,
      "step": 2770
    },
    {
      "epoch": 2.757936507936508,
      "grad_norm": 0.017239375039935112,
      "learning_rate": 4.050925925925926e-06,
      "loss": 0.005,
      "step": 2780
    },
    {
      "epoch": 2.767857142857143,
      "grad_norm": 0.01827467791736126,
      "learning_rate": 3.885582010582011e-06,
      "loss": 0.0039,
      "step": 2790
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.025790797546505928,
      "learning_rate": 3.7202380952380952e-06,
      "loss": 0.0046,
      "step": 2800
    },
    {
      "epoch": 2.7876984126984126,
      "grad_norm": 0.02391517348587513,
      "learning_rate": 3.5548941798941803e-06,
      "loss": 0.0044,
      "step": 2810
    },
    {
      "epoch": 2.7976190476190474,
      "grad_norm": 0.018332839012145996,
      "learning_rate": 3.3895502645502645e-06,
      "loss": 0.0042,
      "step": 2820
    },
    {
      "epoch": 2.807539682539683,
      "grad_norm": 0.03847922757267952,
      "learning_rate": 3.2242063492063495e-06,
      "loss": 0.0046,
      "step": 2830
    },
    {
      "epoch": 2.817460317460317,
      "grad_norm": 0.06500469148159027,
      "learning_rate": 3.058862433862434e-06,
      "loss": 0.0034,
      "step": 2840
    },
    {
      "epoch": 2.8273809523809526,
      "grad_norm": 0.02066919207572937,
      "learning_rate": 2.893518518518519e-06,
      "loss": 0.0038,
      "step": 2850
    },
    {
      "epoch": 2.8373015873015874,
      "grad_norm": 0.03158918768167496,
      "learning_rate": 2.7281746031746034e-06,
      "loss": 0.0042,
      "step": 2860
    },
    {
      "epoch": 2.8472222222222223,
      "grad_norm": 0.015290549024939537,
      "learning_rate": 2.562830687830688e-06,
      "loss": 0.0042,
      "step": 2870
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.01390596479177475,
      "learning_rate": 2.3974867724867727e-06,
      "loss": 0.0033,
      "step": 2880
    },
    {
      "epoch": 2.867063492063492,
      "grad_norm": 0.01626651920378208,
      "learning_rate": 2.2321428571428573e-06,
      "loss": 0.0037,
      "step": 2890
    },
    {
      "epoch": 2.876984126984127,
      "grad_norm": 0.021727805957198143,
      "learning_rate": 2.066798941798942e-06,
      "loss": 0.0037,
      "step": 2900
    },
    {
      "epoch": 2.886904761904762,
      "grad_norm": 0.021728014573454857,
      "learning_rate": 1.9014550264550266e-06,
      "loss": 0.0045,
      "step": 2910
    },
    {
      "epoch": 2.8968253968253967,
      "grad_norm": 0.014716001227498055,
      "learning_rate": 1.7361111111111112e-06,
      "loss": 0.004,
      "step": 2920
    },
    {
      "epoch": 2.9067460317460316,
      "grad_norm": 0.0265350379049778,
      "learning_rate": 1.570767195767196e-06,
      "loss": 0.0042,
      "step": 2930
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.018360836431384087,
      "learning_rate": 1.4054232804232805e-06,
      "loss": 0.0034,
      "step": 2940
    },
    {
      "epoch": 2.9265873015873014,
      "grad_norm": 0.017558202147483826,
      "learning_rate": 1.240079365079365e-06,
      "loss": 0.0036,
      "step": 2950
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.024101780727505684,
      "learning_rate": 1.0747354497354497e-06,
      "loss": 0.0042,
      "step": 2960
    },
    {
      "epoch": 2.946428571428571,
      "grad_norm": 0.019254878163337708,
      "learning_rate": 9.093915343915343e-07,
      "loss": 0.0038,
      "step": 2970
    },
    {
      "epoch": 2.9563492063492065,
      "grad_norm": 0.03446798771619797,
      "learning_rate": 7.440476190476191e-07,
      "loss": 0.0039,
      "step": 2980
    },
    {
      "epoch": 2.9662698412698414,
      "grad_norm": 0.025739222764968872,
      "learning_rate": 5.787037037037037e-07,
      "loss": 0.0037,
      "step": 2990
    },
    {
      "epoch": 2.9761904761904763,
      "grad_norm": 0.02609744295477867,
      "learning_rate": 4.133597883597884e-07,
      "loss": 0.004,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3024,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.7184422829158564e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
